# UGRO HPO: LoRA Fine-tuning with Conditional Parameters
#
# Example search space configuration demonstrating conditional parameter logic.
# Use with: ugro sweep --search-space config/llama_lora_hpo_conditional.yaml

parameters:
  # Optimizer type (controls which other parameters are relevant)
  optimizer_type:
    type: categorical
    choices: ["adam", "adamw", "sgd"]
    default: "adamw"

  # Learning rate (always used)
  learning_rate:
    type: float
    min: 1.0e-5
    max: 1.0e-3
    log: true
    default: 5.0e-4

  # Weight decay (only for AdamW)
  weight_decay:
    type: float
    min: 0.0
    max: 0.1
    default: 0.01
    condition: "optimizer_type == 'adamw'"

  # Momentum (only for SGD)
  momentum:
    type: float
    min: 0.5
    max: 0.99
    default: 0.9
    condition: "optimizer_type == 'sgd'"

  # Beta1 (only for Adam/AdamW)
  beta1:
    type: float
    min: 0.8
    max: 0.999
    default: 0.9
    condition: "optimizer_type in ['adam', 'adamw']"

  # Beta2 (only for Adam/AdamW)
  beta2:
    type: float
    min: 0.98
    max: 0.9999
    default: 0.999
    condition: "optimizer_type in ['adam', 'adamw']"

  # LoRA rank (trade-off: expressiveness vs memory/speed)
  lora_r:
    type: int
    min: 4
    max: 64
    step: 4
    default: 16

  # LoRA scaling factor (typically >= lora_r)
  lora_alpha:
    type: float
    min: 8.0
    max: 128.0
    log: true
    default: 32.0

  # Dropout for LoRA layers
  lora_dropout:
    type: float
    min: 0.0
    max: 0.3
    default: 0.1

  # Batch size (categorical for memory planning)
  batch_size:
    type: categorical
    choices: [8, 16, 32, 64]
    default: 16

  # Warmup ratio (fraction of training for LR warmup)
  warmup_ratio:
    type: float
    min: 0.0
    max: 0.2
    default: 0.05

  # Gradient clipping (only for large batch sizes)
  max_grad_norm:
    type: float
    min: 0.1
    max: 2.0
    default: 1.0
    condition: "batch_size >= 32"

  # Early stopping patience (epochs without improvement)
  early_stopping_patience:
    type: int
    min: 3
    max: 10
    default: 5

# Optimization objectives
objectives:
  - name: eval_loss
    direction: minimize
    weight: 1.0

# Parameter constraints
constraints:
  - "lora_alpha >= lora_r"
  - "learning_rate <= 0.001"
  - "beta1 < beta2"  # Only applies when both are present
